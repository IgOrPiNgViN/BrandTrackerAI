#!/usr/bin/env python3
"""
Ð¡ÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð²ÑÐµÑ… Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð² NLP-Ð°Ð½Ð°Ð»Ð¸Ð·Ð°.
Ð—Ð°Ð¿ÑƒÑÐºÐ°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¸ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð°.

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ:
    py scripts/regenerate_charts.py
    py scripts/regenerate_charts.py --data data/all_reviews.csv
"""

import os
import sys
import argparse

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ñ€Ð½ÐµÐ²ÑƒÑŽ Ð¿Ð°Ð¿ÐºÑƒ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° matplotlib
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'DejaVu Sans'


def setup_directories():
    """Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð¿Ð°Ð¿ÐºÐ¸ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð²"""
    images_dir = os.path.join(project_root, 'reports', 'images')
    os.makedirs(images_dir, exist_ok=True)
    return images_dir


def load_and_analyze_data(data_path: str):
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸ Ð·Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ NLP-Ð°Ð½Ð°Ð»Ð¸Ð·"""
    from nlp.review_analyzer import ReviewAnalyzer
    
    print(f"ðŸ“‚ Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…: {data_path}")
    df = pd.read_csv(data_path, encoding='utf-8-sig')
    print(f"   Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(df)} Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²")
    
    print("â³ Ð—Ð°Ð¿ÑƒÑÐº NLP-Ð°Ð½Ð°Ð»Ð¸Ð·Ð°...")
    analyzer = ReviewAnalyzer()
    rating_col = 'rating' if 'rating' in df.columns else None
    df_analyzed = analyzer.analyze_dataframe(df, text_column='text', rating_column=rating_col)
    
    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸ÐµÑÑ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
    if 'text' in df_analyzed.columns:
        df_analyzed = df_analyzed.drop(columns=['text'])
    if 'rating' in df_analyzed.columns:
        df_analyzed = df_analyzed.drop(columns=['rating'])
    
    # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
    df = df.reset_index()
    df = df.merge(df_analyzed, left_on='index', right_on='original_index', how='left')
    df = df.drop(columns=['index', 'original_index'])
    
    print("âœ… NLP-Ð°Ð½Ð°Ð»Ð¸Ð· Ð·Ð°Ð²ÐµÑ€ÑˆÑ‘Ð½!")
    return df


def generate_chart_01_sentiment(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 1: Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_01_sentiment_distribution.png")
    
    sentiment_counts = df['sentiment'].value_counts()
    sentiment_labels = {'positive': 'ÐŸÐ¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ', 'negative': 'ÐÐµÐ³Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ', 'neutral': 'ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ'}
    sentiment_counts_ru = pd.Series({sentiment_labels.get(k, k): v for k, v in sentiment_counts.items()})
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    colors = ['#2ecc71', '#e74c3c', '#95a5a6']
    
    ax1.pie(sentiment_counts_ru.values, labels=sentiment_counts_ru.index, autopct='%1.1f%%', colors=colors, startangle=90)
    ax1.set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²', fontsize=14, fontweight='bold')
    
    bars = ax2.bar(sentiment_counts_ru.index, sentiment_counts_ru.values, color=colors)
    ax2.set_title('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð² Ð¿Ð¾ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸', fontsize=14, fontweight='bold')
    ax2.set_ylabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²')
    ax2.set_xlabel('Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ')
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_01_sentiment_distribution.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_02_problems(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 2: ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_02_problems_analysis.png")
    
    all_categories = []
    for categories in df['problem_categories']:
        try:
            if categories is None:
                continue
            if isinstance(categories, (list, tuple)) and len(categories) > 0:
                all_categories.extend([c for c in categories if c])
            elif isinstance(categories, str) and categories.strip():
                all_categories.extend([c.strip() for c in categories.split(',') if c.strip()])
        except:
            continue
    
    category_counts = Counter(all_categories)
    category_translation = {
        'ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾_ÐµÐ´Ñ‹': 'ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ ÐµÐ´Ñ‹', 'Ð¾Ð±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ': 'ÐžÐ±ÑÐ»ÑƒÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ',
        'Ñ‡Ð¸ÑÑ‚Ð¾Ñ‚Ð°': 'Ð§Ð¸ÑÑ‚Ð¾Ñ‚Ð°', 'Ñ†ÐµÐ½Ñ‹': 'Ð¦ÐµÐ½Ñ‹', 'Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ðµ': 'ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ',
        'Ð°Ñ‚Ð¼Ð¾ÑÑ„ÐµÑ€Ð°': 'ÐÑ‚Ð¼Ð¾ÑÑ„ÐµÑ€Ð°', 'Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ': 'Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ', 'Ñ€Ð°Ð·Ð¼ÐµÑ€_Ð¿Ð¾Ñ€Ñ†Ð¸Ð¹': 'Ð Ð°Ð·Ð¼ÐµÑ€ Ð¿Ð¾Ñ€Ñ†Ð¸Ð¹'
    }
    category_counts_ru = {category_translation.get(k, k): v for k, v in category_counts.items()}
    
    if category_counts_ru:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        top_categories = dict(sorted(category_counts_ru.items(), key=lambda x: x[1], reverse=True)[:10])
        
        y_pos = np.arange(len(top_categories))
        ax1.barh(y_pos, list(top_categories.values()), color='#e74c3c')
        ax1.set_yticks(y_pos)
        ax1.set_yticklabels(list(top_categories.keys()))
        ax1.set_xlabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ð¹')
        ax1.set_title('Ð¢Ð¾Ð¿ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ð² Ð¾Ñ‚Ð·Ñ‹Ð²Ð°Ñ…', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()
        
        ax2.pie(top_categories.values(), labels=top_categories.keys(), autopct='%1.1f%%', startangle=90)
        ax2.set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{images_dir}/nlp_02_problems_analysis.png', dpi=150, bbox_inches='tight', facecolor='white')
        plt.close()


def generate_chart_03_scores(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 3: Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾Ñ†ÐµÐ½Ð¾Ðº Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_03_sentiment_scores.png")
    
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.hist(df['sentiment_score'], bins=30, color='#3498db', edgecolor='black', alpha=0.7)
    ax.axvline(df['sentiment_score'].mean(), color='red', linestyle='--', linewidth=2, 
               label=f'Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ: {df["sentiment_score"].mean():.2f}')
    ax.axvline(0, color='black', linestyle='-', linewidth=1, alpha=0.3)
    ax.set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾Ñ†ÐµÐ½Ð¾Ðº Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸', fontsize=14, fontweight='bold')
    ax.set_xlabel('ÐžÑ†ÐµÐ½ÐºÐ° Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ (Ð¾Ñ‚ -1 Ð´Ð¾ +1)')
    ax.set_ylabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_03_sentiment_scores.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_04_link(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 4: Ð¡Ð²ÑÐ·ÑŒ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_04_sentiment_problems_link.png")
    
    sentiment_labels = {'positive': 'ÐŸÐ¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ', 'negative': 'ÐÐµÐ³Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ', 'neutral': 'ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ'}
    
    fig, ax = plt.subplots(figsize=(12, 6))
    pivot_data = pd.crosstab(df['sentiment'], df['has_problems'], normalize='index') * 100
    pivot_data.index = [sentiment_labels.get(idx, idx) for idx in pivot_data.index]
    pivot_data.columns = ['Ð‘ÐµÐ· Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', 'Ð¡ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ð¼Ð¸']
    
    pivot_data.plot(kind='bar', ax=ax, color=['#2ecc71', '#e74c3c'], width=0.8)
    ax.set_title('Ð¡Ð²ÑÐ·ÑŒ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¸ Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', fontsize=14, fontweight='bold')
    ax.set_xlabel('Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ')
    ax.set_ylabel('ÐŸÑ€Ð¾Ñ†ÐµÐ½Ñ‚ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð² (%)')
    ax.legend(title='')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_04_sentiment_problems_link.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_05_rating(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 5: ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð¾ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð°Ð¼"""
    if 'rating' not in df.columns or not df['rating'].notna().any():
        print("âš ï¸  ÐŸÑ€Ð¾Ð¿ÑƒÑÐº nlp_05_rating_analysis.png (Ð½ÐµÑ‚ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð¾Ð²)")
        return
    
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_05_rating_analysis.png")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
    
    rating_counts = df['rating'].value_counts().sort_index()
    ax1.bar(rating_counts.index, rating_counts.values, color='#f39c12', edgecolor='black')
    ax1.set_title('Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð¾Ð²', fontsize=12, fontweight='bold')
    ax1.set_xlabel('Ð ÐµÐ¹Ñ‚Ð¸Ð½Ð³')
    ax1.set_ylabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²')
    
    rating_sentiment = pd.crosstab(df['rating'], df['sentiment'])
    rating_sentiment.plot(kind='bar', ax=ax2, color=['#e74c3c', '#95a5a6', '#2ecc71'])
    ax2.set_title('Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ Ð¿Ð¾ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð°Ð¼', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Ð ÐµÐ¹Ñ‚Ð¸Ð½Ð³')
    ax2.set_ylabel('ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð²')
    ax2.legend(['ÐÐµÐ³Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ', 'ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ðµ', 'ÐŸÐ¾Ð·Ð¸Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ'])
    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)
    
    avg_sentiment_by_rating = df.groupby('rating')['sentiment_score'].mean()
    ax3.plot(avg_sentiment_by_rating.index, avg_sentiment_by_rating.values, marker='o', linewidth=2, markersize=8, color='#3498db')
    ax3.set_title('Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¾Ñ†ÐµÐ½ÐºÐ° Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð°Ð¼', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Ð ÐµÐ¹Ñ‚Ð¸Ð½Ð³')
    ax3.set_ylabel('Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ð¾Ñ†ÐµÐ½ÐºÐ° Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸')
    ax3.grid(True, alpha=0.3)
    ax3.axhline(0, color='black', linestyle='--', alpha=0.3)
    
    problems_by_rating = df.groupby('rating')['problems_count'].mean()
    ax4.bar(problems_by_rating.index, problems_by_rating.values, color='#e74c3c', edgecolor='black')
    ax4.set_title('Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼ Ð¿Ð¾ Ñ€ÐµÐ¹Ñ‚Ð¸Ð½Ð³Ð°Ð¼', fontsize=12, fontweight='bold')
    ax4.set_xlabel('Ð ÐµÐ¹Ñ‚Ð¸Ð½Ð³')
    ax4.set_ylabel('Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_05_rating_analysis.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_06_correlation(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 6: ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð°"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_06_correlation_matrix.png")
    
    corr_data = df.copy()
    sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}
    corr_data['sentiment_numeric'] = corr_data['sentiment'].map(sentiment_map)
    
    numeric_cols = ['sentiment_score', 'sentiment_confidence', 'problems_count', 'has_problems', 'sentiment_numeric']
    if 'rating' in corr_data.columns:
        numeric_cols.append('rating')
    
    corr_matrix = corr_data[numeric_cols].corr()
    
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, square=True, linewidths=1, ax=ax)
    ax.set_title('ÐšÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð°Ñ‚Ñ€Ð¸Ñ†Ð° Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_06_correlation_matrix.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_07_classification(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 7: ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_07_classification.png")
    
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
    from sklearn.metrics import confusion_matrix, accuracy_score
    from sklearn.preprocessing import StandardScaler
    
    X = df[['sentiment_score', 'sentiment_confidence']].copy()
    if 'rating' in df.columns:
        X['rating'] = df['rating'].fillna(df['rating'].median())
    X = X.fillna(X.mean())
    y = df['has_problems'].astype(int)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)
    }
    
    results = {}
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        results[name] = {'accuracy': accuracy_score(y_test, y_pred), 'predictions': y_pred}
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    for idx, (name, result) in enumerate(results.items()):
        cm = confusion_matrix(y_test, result['predictions'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                    xticklabels=['Ð‘ÐµÐ· Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', 'Ð¡ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ð¼Ð¸'],
                    yticklabels=['Ð‘ÐµÐ· Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', 'Ð¡ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°Ð¼Ð¸'])
        axes[idx].set_title(f'{name}\nÐ¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {result["accuracy"]:.3f}', fontweight='bold')
        axes[idx].set_ylabel('Ð˜ÑÑ‚Ð¸Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ')
        axes[idx].set_xlabel('ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_07_classification.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_08_clustering(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 8: ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_08_clustering.png")
    
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    from sklearn.metrics import silhouette_score
    
    X_cluster = df[['sentiment_score', 'sentiment_confidence', 'problems_count']].copy()
    X_cluster = X_cluster.fillna(X_cluster.mean())
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_cluster)
    
    silhouette_scores = []
    K_range = range(2, 11)
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X_scaled)
        silhouette_scores.append(silhouette_score(X_scaled, labels))
    
    optimal_k = K_range[np.argmax(silhouette_scores)]
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X_scaled)
    
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    scatter = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6, s=50)
    axes[0].set_title('ÐšÐ»Ð°ÑÑ‚ÐµÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð² (PCA)', fontweight='bold')
    axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
    axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
    plt.colorbar(scatter, ax=axes[0], label='ÐšÐ»Ð°ÑÑ‚ÐµÑ€')
    
    axes[1].plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8)
    axes[1].axvline(optimal_k, color='r', linestyle='--', label=f'ÐžÐ¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ðµ k={optimal_k}')
    axes[1].set_title('ÐŸÐ¾Ð¸ÑÐº Ð¾Ð¿Ñ‚Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‡Ð¸ÑÐ»Ð° ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²', fontweight='bold')
    axes[1].set_xlabel('Ð§Ð¸ÑÐ»Ð¾ ÐºÐ»Ð°ÑÑ‚ÐµÑ€Ð¾Ð²')
    axes[1].set_ylabel('Silhouette Score')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_08_clustering.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_09_ensemble(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 9: ÐÐ½ÑÐ°Ð¼Ð±Ð»ÐµÐ²Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ"""
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_09_ensemble_learning.png")
    
    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    X = df[['sentiment_score', 'sentiment_confidence']].copy()
    if 'rating' in df.columns:
        X['rating'] = df['rating'].fillna(df['rating'].median())
    X = X.fillna(X.mean())
    y = df['has_problems'].astype(int)
    
    base_estimator = DecisionTreeClassifier(max_depth=5, random_state=42)
    
    ensemble_models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
        'Bagging': BaggingClassifier(estimator=base_estimator, n_estimators=50, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5),
        'AdaBoost': AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42)
    }
    
    results_ensemble = {}
    for name, model in ensemble_models.items():
        cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
        model.fit(X, y)
        y_pred = model.predict(X)
        results_ensemble[name] = {
            'cv_mean': cv_scores.mean(), 'cv_std': cv_scores.std(),
            'accuracy': accuracy_score(y, y_pred), 'precision': precision_score(y, y_pred),
            'recall': recall_score(y, y_pred), 'f1': f1_score(y, y_pred)
        }
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    names = list(results_ensemble.keys())
    cv_means = [results_ensemble[n]['cv_mean'] for n in names]
    cv_stds = [results_ensemble[n]['cv_std'] for n in names]
    
    axes[0, 0].barh(names, cv_means, xerr=cv_stds, capsize=5)
    axes[0, 0].set_title('Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ (Cross-Validation)', fontweight='bold')
    axes[0, 0].set_xlabel('Ð¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ')
    axes[0, 0].grid(True, alpha=0.3, axis='x')
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    x = np.arange(len(names))
    width = 0.2
    for i, metric in enumerate(metrics):
        values = [results_ensemble[n][metric] for n in names]
        axes[0, 1].bar(x + i*width, values, width, label=metric.capitalize())
    axes[0, 1].set_title('ÐœÐµÑ‚Ñ€Ð¸ÐºÐ¸ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸', fontweight='bold')
    axes[0, 1].set_xticks(x + width * 1.5)
    axes[0, 1].set_xticklabels(names, rotation=45, ha='right')
    axes[0, 1].legend()
    
    comparison_df = pd.DataFrame(results_ensemble).T[['cv_mean', 'accuracy', 'precision', 'recall', 'f1']]
    sns.heatmap(comparison_df, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[1, 0])
    axes[1, 0].set_title('Ð¡Ñ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº', fontweight='bold')
    
    best_model = max(results_ensemble.items(), key=lambda x: x[1]['cv_mean'])
    axes[1, 1].text(0.5, 0.5, f'ðŸ† Ð›ÑƒÑ‡ÑˆÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ:\n{best_model[0]}\n\nÐ¢Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ: {best_model[1]["cv_mean"]:.3f}',
                    ha='center', va='center', fontsize=14, fontweight='bold',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    axes[1, 1].axis('off')
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_09_ensemble_learning.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_10_association(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 10: ÐÑÑÐ¾Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°"""
    try:
        from mlxtend.frequent_patterns import apriori, association_rules
        from mlxtend.preprocessing import TransactionEncoder
    except ImportError:
        print("âš ï¸  ÐŸÑ€Ð¾Ð¿ÑƒÑÐº nlp_10_association_rules.png (Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ mlxtend)")
        return
    
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_10_association_rules.png")
    
    transactions = []
    for categories in df['problem_categories']:
        transaction = []
        try:
            if categories is None:
                transactions.append([])
                continue
            if isinstance(categories, (list, tuple)):
                transaction = [c for c in categories if c]
            elif isinstance(categories, str):
                transaction = [c.strip() for c in categories.split(',') if c.strip()]
        except:
            transaction = []
        transactions.append(transaction)
    
    transactions_filtered = [t for t in transactions if len(t) > 0]
    
    if len(transactions_filtered) < 10:
        print("âš ï¸  ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°ÑÑÐ¾Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð»")
        return
    
    te = TransactionEncoder()
    te_ary = te.fit(transactions_filtered).transform(transactions_filtered)
    df_transactions = pd.DataFrame(te_ary, columns=te.columns_)
    
    frequent_itemsets = apriori(df_transactions, min_support=0.02, use_colnames=True)
    
    if len(frequent_itemsets) == 0:
        print("âš ï¸  ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡Ð°ÑÑ‚Ñ‹Ñ… Ð½Ð°Ð±Ð¾Ñ€Ð¾Ð²")
        return
    
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.3)
    
    if len(rules) == 0:
        print("âš ï¸  ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð°ÑÑÐ¾Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ñ… Ð¿Ñ€Ð°Ð²Ð¸Ð»")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    axes[0].scatter(rules['support'], rules['confidence'], s=rules['lift']*50, alpha=0.6, c=rules['lift'], cmap='viridis')
    axes[0].set_xlabel('Support')
    axes[0].set_ylabel('Confidence')
    axes[0].set_title('ÐÑÑÐ¾Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð°Ð²Ð¸Ð»Ð°\n(Ñ€Ð°Ð·Ð¼ÐµÑ€ = lift)', fontweight='bold')
    axes[0].grid(True, alpha=0.3)
    
    top_rules = rules.nlargest(10, 'lift')
    y_pos = np.arange(len(top_rules))
    axes[1].barh(y_pos, top_rules['lift'], color='#3498db')
    axes[1].set_yticks(y_pos)
    axes[1].set_yticklabels([f"{', '.join(list(r['antecedents']))} â†’ {', '.join(list(r['consequents']))}" 
                             for _, r in top_rules.iterrows()], fontsize=8)
    axes[1].set_xlabel('Lift')
    axes[1].set_title('Ð¢Ð¾Ð¿-10 Ð¿Ñ€Ð°Ð²Ð¸Ð» Ð¿Ð¾ Lift', fontweight='bold')
    axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_10_association_rules.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def generate_chart_11_forecast(df, images_dir):
    """Ð“Ñ€Ð°Ñ„Ð¸Ðº 11: ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ"""
    if 'date' not in df.columns:
        print("âš ï¸  ÐŸÑ€Ð¾Ð¿ÑƒÑÐº nlp_11_forecast.png (Ð½ÐµÑ‚ Ð´Ð°Ñ‚)")
        return
    
    print("ðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ: nlp_11_forecast.png")
    
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import PolynomialFeatures
    
    df_copy = df.copy()
    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce')
    df_copy = df_copy.dropna(subset=['date'])
    
    if len(df_copy) < 10:
        print("âš ï¸  ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")
        return
    
    df_copy['year_month'] = df_copy['date'].dt.to_period('M')
    monthly_stats = df_copy.groupby('year_month').agg({
        'sentiment_score': 'mean', 'problems_count': 'mean', 'has_problems': 'mean'
    }).reset_index()
    monthly_stats['month_index'] = range(len(monthly_stats))
    
    future_months = 12
    X = monthly_stats[['month_index']].values
    
    predictions = {}
    for metric_name, col in [('Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', 'sentiment_score'), ('ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹', 'problems_count')]:
        y_data = monthly_stats[col].values
        lr = LinearRegression()
        lr.fit(X, y_data)
        future_indices = np.array(range(len(monthly_stats), len(monthly_stats) + future_months)).reshape(-1, 1)
        predictions[metric_name] = {'linear': lr.predict(future_indices), 'actual': y_data}
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    
    last_date = monthly_stats['year_month'].iloc[-1]
    future_dates = [str(last_date + i) for i in range(1, future_months + 1)]
    
    for idx, (metric_name, ylabel, ax) in enumerate([('Ð¢Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', 'Ð¡Ñ€ÐµÐ´Ð½ÑÑ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', axes[0]), 
                                                       ('ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹', 'Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÐºÐ¾Ð»-Ð²Ð¾ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼', axes[1])]):
        pred_data = predictions[metric_name]
        historical_indices = range(len(monthly_stats))
        ax.plot(historical_indices, pred_data['actual'], 'o-', label='Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ', linewidth=2, color='blue')
        future_indices_plot = range(len(monthly_stats), len(monthly_stats) + future_months)
        ax.plot(future_indices_plot, pred_data['linear'], 's--', label='Ð›Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·', linewidth=2, color='green')
        ax.axvline(len(monthly_stats) - 0.5, color='gray', linestyle=':', linewidth=2, alpha=0.5)
        ax.set_xlabel('ÐŸÐµÑ€Ð¸Ð¾Ð´ (Ð¼ÐµÑÑÑ†Ñ‹)')
        ax.set_ylabel(ylabel)
        ax.set_title(f'ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð· {metric_name.lower()} Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ðµ 12 Ð¼ÐµÑÑÑ†ÐµÐ²', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{images_dir}/nlp_11_forecast.png', dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='ÐŸÐµÑ€ÐµÐ³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð² NLP-Ð°Ð½Ð°Ð»Ð¸Ð·Ð°')
    parser.add_argument('--data', type=str, default='data/all_reviews.csv', 
                        help='ÐŸÑƒÑ‚ÑŒ Ðº Ñ„Ð°Ð¹Ð»Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: data/all_reviews.csv)')
    args = parser.parse_args()
    
    data_path = os.path.join(project_root, args.data)
    
    print("=" * 60)
    print("ðŸ”„ ÐŸÐ•Ð Ð•Ð“Ð•ÐÐ•Ð ÐÐ¦Ð˜Ð¯ Ð“Ð ÐÐ¤Ð˜ÐšÐžÐ’ NLP-ÐÐÐÐ›Ð˜Ð—Ð")
    print("=" * 60)
    
    images_dir = setup_directories()
    print(f"ðŸ“ ÐŸÐ°Ð¿ÐºÐ° Ð´Ð»Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð²: {images_dir}")
    
    df = load_and_analyze_data(data_path)
    
    print("\nðŸ“Š Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð³Ñ€Ð°Ñ„Ð¸ÐºÐ¾Ð²...")
    
    generate_chart_01_sentiment(df, images_dir)
    generate_chart_02_problems(df, images_dir)
    generate_chart_03_scores(df, images_dir)
    generate_chart_04_link(df, images_dir)
    generate_chart_05_rating(df, images_dir)
    generate_chart_06_correlation(df, images_dir)
    generate_chart_07_classification(df, images_dir)
    generate_chart_08_clustering(df, images_dir)
    generate_chart_09_ensemble(df, images_dir)
    generate_chart_10_association(df, images_dir)
    generate_chart_11_forecast(df, images_dir)
    
    print("\n" + "=" * 60)
    print("âœ… Ð’Ð¡Ð• Ð“Ð ÐÐ¤Ð˜ÐšÐ˜ Ð£Ð¡ÐŸÐ•Ð¨ÐÐž Ð¡Ð“Ð•ÐÐ•Ð Ð˜Ð ÐžÐ’ÐÐÐ«!")
    print(f"ðŸ“ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ñ‹ Ð²: {images_dir}")
    print("=" * 60)


if __name__ == '__main__':
    main()


